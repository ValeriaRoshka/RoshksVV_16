{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValeriaRoshka/RoshksVV_16/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DIq6U8bFkdEC",
        "outputId": "dc745d82-2a49-4bae-c8ac-a8d6d8cbe791"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-34eeb238d30c>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Выполнение обнаружения объектов и получение списка ошибок\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Вывод количества классов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-34eeb238d30c>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(image_path, thresh)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Загрузка модели и получение списка классов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo/yolov3.weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yolo/yolov3.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo/coco.names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/dnn/src/darknet/darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolo/yolov3.cfg in function 'readNetFromDarknet'\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def detect_objects(image_path, thresh):\n",
        "    # Загрузка изображения\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Загрузка модели и получение списка классов\n",
        "    net = cv2.dnn.readNet(\"yolo/yolov3.weights\", \"yolo/yolov3.cfg\")\n",
        "    classes = []\n",
        "    with open(\"yolo/coco.names\", \"r\") as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    height, width, channels = image.shape\n",
        "\n",
        "    # Создание входного бинарного блоба для модели\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "\n",
        "    # Задание входного бинарного блоба модели\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Получение выходных слоев модели\n",
        "    layer_names = net.getLayerNames()\n",
        "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    # Прямой проход модели\n",
        "    outputs = net.forward(output_layers)\n",
        "\n",
        "    # Обработка выходных слоев модели\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    class_ids = []\n",
        "    for output in outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > thresh:\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    # Определение границ обнаруженных объектов\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    # Формирование списка ошибок классификации и локализации для каждого обнаруженного объекта\n",
        "    errors = []\n",
        "    for i in range(len(boxes)):\n",
        "        x, y, w, h = boxes[i]\n",
        "        if i in indices:\n",
        "            if classes[class_ids[i]] == \"cat\":\n",
        "                class_error = 0\n",
        "            else:\n",
        "                class_error = 1\n",
        "            if w / h >= 0.7 and w / h <= 1.3:\n",
        "                localization_error = 0\n",
        "            else:\n",
        "                localization_error = 1\n",
        "            errors.append({\"class_error\": class_error, \"localization_error\": localization_error})\n",
        "        else:\n",
        "            errors.append({\"class_error\": None, \"localization_error\": None})\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "image_path = \"cat.jpg\"\n",
        "thresh = 0.1\n",
        "\n",
        "# Выполнение обнаружения объектов и получение списка ошибок\n",
        "errors = detect_objects(image_path, thresh)\n",
        "\n",
        "# Вывод количества классов\n",
        "print(f\"Number of classes: {len(set(classes))}\")\n",
        "\n",
        "# Вывод ошибок классификации и локализации для каждого объекта в список в виде словаря\n",
        "for i in range(len(errors)):\n",
        "    print(f\"Object {i}: {errors[i]}\")\n",
        "\n",
        "# Отображение изображения с рамками вокруг найденных объектов при различных порогах\n",
        "for t in range(1, 10):\n",
        "    t /= 10.0\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    class_ids = []\n",
        "    for output in outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > t:\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "    for i in indices:\n",
        "        i = i[0]\n",
        "        box = boxes[i]\n",
        "        x, y, w, h = box\n",
        "        color = colors[class_ids[i]]\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "        cv2.putText(image, f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    cv2.imshow(\"Image\", image)\n",
        "    cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yPnQo7SdEFPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# загрузка модели YOLO\n",
        "net = cv2.dnn.readNet('yolo/yolov3.weights', 'yolo/yolov3.cfg')\n",
        "\n",
        "# список классов\n",
        "classes = []\n",
        "with open('/content/yolo/coco.names', 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# порог вероятности обнаружения объекта\n",
        "conf_threshold = 0.5\n",
        "\n",
        "# порог вероятности классификации объекта\n",
        "nms_threshold = 0.4\n",
        "\n",
        "# загрузка изображения\n",
        "img = cv2.imread('image.jpg')\n",
        "\n",
        "# получение размеров изображения\n",
        "height, width, channels = img.shape\n",
        "\n",
        "# создание блоба изображения\n",
        "blob = cv2.dnn.blobFromImage(img, 1 / 255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
        "\n",
        "# передача блоба через сеть\n",
        "net.setInput(blob)\n",
        "\n",
        "# получение и фильтрация результатов детекции\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "outs = net.forward(output_layers)\n",
        "class_ids = []\n",
        "confidences = []\n",
        "boxes = []\n",
        "for out in outs:\n",
        "    for detection in out:\n",
        "        scores = detection[5:]\n",
        "        class_id = np.argmax(scores)\n",
        "        confidence = scores[class_id]\n",
        "        if confidence > conf_threshold:\n",
        "            center_x = int(detection[0] * width)\n",
        "            center_y = int(detection[1] * height)\n",
        "            w = int(detection[2] * width)\n",
        "            h = int(detection[3] * height)\n",
        "            x = int(center_x - w / 2)\n",
        "            y = int(center_y - h / 2)\n",
        "            class_ids.append(class_id)\n",
        "            confidences.append(float(confidence))\n",
        "            boxes.append([x, y, w, h])\n",
        "\n",
        "# применение фильтра немаксимума для устранения дублированных рамок\n",
        "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "\n",
        "# отображение рамок вокруг найденных объектов\n",
        "for i in indices:\n",
        "    i = i[0]\n",
        "    box = boxes[i]\n",
        "    x = box[0]\n",
        "    y = box[1]\n",
        "    w = box[2]\n",
        "    h = box[3]\n",
        "    label = str(classes[class_ids[i]])\n",
        "    confidence = confidences[i]\n",
        "    color = (0, 255, 0)\n",
        "    cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "    cv2.putText(img, label + ' ' + str(round(confidence, 2)), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "# вывод количества классов\n",
        "print(\"Количество классов: \", len(classes))\n",
        "\n",
        "# вывод ошибок классификации и ошибок локализации для каждого объекта в список в виде словаря\n",
        "errors = {}\n",
        "for i in indices:\n",
        "    i = i[0]\n",
        "    box = boxes[i]\n",
        "    x = box[0]\n",
        "    y = box[1]\n",
        "    w = box[2]\n",
        "    h = box[3]\n",
        "    label = str(classes[class_ids[i]])\n",
        "\n",
        "# Задаем порог вероятности\n",
        "threshold = 0.5\n",
        "\n",
        "# Проходимся по всем обнаруженным объектам\n",
        "for i, detection in enumerate(detections):\n",
        "    # Извлекаем вероятность и индекс класса для текущего объекта\n",
        "    scores = detection[5:]\n",
        "    class_id = np.argmax(scores)\n",
        "    confidence = scores[class_id]\n",
        "    \n",
        "    # Отбрасываем обнаружения с низкой вероятностью\n",
        "    if confidence > threshold:\n",
        "        # Получаем размеры изображения\n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        # Извлекаем координаты ограничивающей рамки для объекта\n",
        "        box = detection[:4] * np.array([w, h, w, h])\n",
        "        (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "        \n",
        "        # Вычисляем координаты верхнего левого угла рамки\n",
        "        x = int(centerX - (width / 2))\n",
        "        y = int(centerY - (height / 2))\n",
        "        \n",
        "        # Рисуем рамку вокруг объекта на изображении\n",
        "        color = [int(c) for c in COLORS[class_id]]\n",
        "        cv2.rectangle(image, (x, y), (x + int(width), y + int(height)), color, 2)\n",
        "        \n",
        "        # Выводим метку класса и вероятность на изображении\n",
        "        text = \"{}: {:.4f}\".format(LABELS[class_id], confidence)\n",
        "        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        \n",
        "# Показываем изображение с нарисованными рамками вокруг найденных объектов\n",
        "cv2.imshow(\"Object Detection\", image)\n",
        "cv2.waitKey(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "XmlKYTShKQpu",
        "outputId": "b089baf9-72c9-4cff-e1da-5b9c299154af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-94e6504783e8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# загрузка модели YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolo/yolov3.weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolo/yolov3.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# список классов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/dnn/src/darknet/darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolo/yolov3.cfg in function 'readNetFromDarknet'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "      self.xmin = xmin\n",
        "      self.ymin = ymin\n",
        "      self.xmax = xmax\n",
        "      self.ymax = ymax\n",
        "      self.objness = objness\n",
        "      self.classes = classes\n",
        "      self.label = -1\n",
        "      self.score = -1\n",
        "\n",
        "def get_label(self):\n",
        "    if self.label == -1:\n",
        "      self.label = np.argmax(self.classes)\n",
        "\n",
        "    return self.label\n",
        "\n",
        "def get_score(self):\n",
        "    if self.score == -1:\n",
        "      self.score = self.classes[self.get_label()]\n",
        "\n",
        "    return self.score\n",
        "\n",
        "def _sigmoid(x):\n",
        "  return 1. / (1. + np.exp(-x))\n",
        "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
        "  grid_h, grid_w = netout.shape[:2]\n",
        "  nb_box = 3\n",
        "  netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "  nb_class = netout.shape[-1] - 5\n",
        "  boxes = []\n",
        "  netout[..., : 2] = _sigmoid(netout[..., : 2])\n",
        "  netout[..., 4: ] = _sigmoid(netout[..., 4: ])\n",
        "  netout[..., 5: ] = netout[..., 4][..., np.newaxis] * netout[..., 5: ]\n",
        "  netout[..., 5: ] *= netout[..., 5: ] > obj_thresh\n",
        "\n",
        "  for i in range(grid_h*grid_w):\n",
        "    row = i / grid_w\n",
        "    col = i % grid_w\n",
        "    for b in range(nb_box): \n",
        "      objectness = netout[int(row)][int(col)][b][4]\n",
        "      if(objectness.all() <= obj_thresh): continue  \n",
        "      x, y, w, h = netout[int(row)][int(col)][b][: 4]\n",
        "      x = (col + x) / grid_w# center position, unit: image width\n",
        "      y = (row + y) / grid_h# center position, unit: image height\n",
        "      w = anchors[2 * b + 0] * np.exp(w) / net_w# unit: image width\n",
        "      h = anchors[2 * b + 1] * np.exp(h) / net_h# unit: image height# last elements are class probabilities\n",
        "      classes = netout[int(row)][col][b][5: ]\n",
        "      box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness,classes)\n",
        "      boxes.append(box)\n",
        "    return boxes\n",
        "\n",
        "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
        "  new_w, new_h = net_w, net_h\n",
        "  for i in range(len(boxes)):\n",
        "    x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) /net_w\n",
        "    y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) /net_h\n",
        "    boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "    boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "    boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "    boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "  x1, x2 = interval_a\n",
        "  x3, x4 = interval_b\n",
        "  if x3 < x1:\n",
        "    if x4 < x1:\n",
        "      return 0\n",
        "    else :\n",
        "      return min(x2, x4) - x1\n",
        "  else :\n",
        "    if x2 < x3:\n",
        "      return 0\n",
        "    else :\n",
        "      return min(x2, x4) - x3\n",
        "def bbox_iou(box1, box2):\n",
        "  intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin,box2.xmax])\n",
        "  intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin,box2.ymax])\n",
        "  intersect = intersect_w * intersect_h\n",
        "  w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin\n",
        "  w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin\n",
        "  union = w1 * h1 + w2 * h2 - intersect\n",
        "  return float(intersect) / union\n",
        "def do_nms(boxes, nms_thresh):\n",
        "  if len(boxes) > 0:\n",
        "    nb_class = len(boxes[0].classes)\n",
        "  else :\n",
        "    return\n",
        "  for c in range(nb_class):\n",
        "    sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "    for i in range(len(sorted_indices)):\n",
        "      index_i = sorted_indices[i]\n",
        "      if boxes[index_i].classes[c] == 0: continue\n",
        "      for j in range(i + 1, len(sorted_indices)):\n",
        "        index_j = sorted_indices[j]\n",
        "        if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "          boxes[index_j].classes[c] = 0 \n",
        "          # load and prepare an image\n",
        "def load_image_pixels(filename, shape): #load the image to get its shape\n",
        "  image = load_img(filename)\n",
        "  width, height = image.size# load the image with the required size\n",
        "  image = load_img(filename, target_size = shape)# convert to numpy array\n",
        "  image = img_to_array(image)# scale pixel values to[0, 1]\n",
        "  image = image.astype('float32')\n",
        "  image /= 255.0\n",
        "  # add a dimension so that we have one sample\n",
        "  image = expand_dims(image, 0)\n",
        "  return image, width, height\n",
        "  # get all of the results above a threshold\n",
        "def get_boxes(boxes, labels, thresh):\n",
        "  v_boxes, v_labels, v_scores = list(), list(), list()\n",
        "  # enumerate all boxes\n",
        "  for box in boxes: #enumerate all possible labels\n",
        "    for i in range(len(labels)): \n",
        "      if box.classes[i] > thresh:\n",
        "        v_boxes.append(box)\n",
        "        v_labels.append(labels[i])\n",
        "        v_scores.append(box.classes[i] * 100)\n",
        "  return v_boxes, v_labels, v_scores# draw all results\n",
        "def draw_boxes(filename, v_boxes, v_labels, v_scores): #load the image\n",
        "  data = pyplot.imread(filename)# plot the image\n",
        "  pyplot.imshow(data)# get the context\n",
        "  ax = pyplot.gca()# plot each box\n",
        "  for i in range(len(v_boxes)):\n",
        "    box = v_boxes[i]# get coordinates\n",
        "    y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax# calculate width and height of the box\n",
        "    width, height = x2 - x1, y2 - y1# create the shape\n",
        "    rect = Rectangle((x1, y1), width, height, fill = False, color = 'white')# draw the box\n",
        "    ax.add_patch(rect)# draw text and score in top left corner\n",
        "    label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
        "    pyplot.text(x1, y1, label, color = 'white')# show the plot\n",
        "  pyplot.show()# load yolov3 model\n",
        "config_file = \"yolov3.cfg\"\n",
        "weights_file = \"yolov3.weights\"\n",
        "net = darknet.load_net_custom(config_file.encode(\"ascii\"), weights_file.encode(\"ascii\"), 0, 1)\n",
        "\n",
        "model = load_model('model.h5')# define the expected input shape\n",
        "input_w, input_h = 416, 416\n",
        "photo_filename = 'zebra.jpg'# load and prepare image\n",
        "image, image_w, image_h = load_image_pixels(photo_filename, (input_w,input_h))# make prediction\n",
        "yhat = model.predict(image)# summarize the shape of the list of arrays\n",
        "print([a.shape for a in yhat])# define the anchors\n",
        "anchors = [[116, 90, 156, 198, 373, 326],[30, 61, 62, 45, 59, 119],[10, 13, 16, 30, 33, 23]]\n",
        "class_threshold = 0.6\n",
        "boxes = list()\n",
        "for i in range(len(yhat)): #decode the output of the network\n",
        "  boxes += decode_netout(yhat[i][0], anchors[i], class_threshold,input_h, input_w)\n",
        "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n",
        "do_nms(boxes, 0.5)# define the labels\n",
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\",\"train\", \"truck\",\n",
        "  \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\",\"parking meter\", \"bench\",\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
        "  \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "  \"frisbee\", \"skis\", \"snowboard\",\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\"skateboard\", \"surfboard\",\n",
        "  \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\"knife\", \"spoon\", \"bowl\",\"banana\",\n",
        "  \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\",\"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
        "  \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\",\"toilet\", \"tvmonitor\", \"laptop\",\n",
        "  \"mouse\",\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\"toaster\", \"sink\",\n",
        "  \"refrigerator\",\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\"hair drier\", \"toothbrush\"]# get the details of the detected objects\n",
        "v_boxes, v_labels, v_scores = get_boxes(boxes, labels,class_threshold)# summarize what we found\n",
        "for i in range(len(v_boxes)):\n",
        "  print(v_labels[i], v_scores[i])# draw what we found\n",
        "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "5K7AXzCOxVU7",
        "outputId": "097c4eeb-2036-454a-b711-1ac60abfd001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e2b872d4ddb0>\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# load yolov3 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/sample_data/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# define the expected input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0minput_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mphoto_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zebra.jpg'\u001b[0m\u001b[0;31m# load and prepare image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at content/sample_data/model.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import numpy as np\n",
        "\n",
        "# Загрузка сети VGG16\n",
        "model = VGG16()\n",
        "\n",
        "# Загрузка изображения и изменение его размера до 224x224 пикселей\n",
        "image = load_img( '/content/sample_data/cat.jpg', target_size=(224, 224))\n",
        "\n",
        "# Преобразование изображения в массив numpy\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Добавление дополнительного измерения для обработки сетью VGG16\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "# Предварительная обработка изображения для VGG16\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# Применение сети VGG16 для классификации изображения\n",
        "predictions = model.predict(image)\n",
        "\n",
        "# Декодирование предсказаний\n",
        "label = decode_predictions(predictions)\n",
        "\n",
        "# Вывод результатов\n",
        "print(label[0][0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtScfMm3n41N",
        "outputId": "a09af12c-6505-4475-f802-71edea932cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 976ms/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "African_elephant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import numpy as np\n",
        "import os\n",
        "# Загрузка сети VGG16\n",
        "model = VGG16()\n",
        "for file in os.listdir('/content/sample_data/images'):\n",
        "# Загрузка изображения и изменение его размера до 224x224 пикселей\n",
        "  image = load_img('/content/sample_data/images/'+file, target_size=(224, 224))\n",
        "\n",
        "# Преобразование изображения в массив numpy\n",
        "  image = img_to_array(image)\n",
        "\n",
        "# Добавление дополнительного измерения для обработки сетью VGG16\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "# Предварительная обработка изображения для VGG16\n",
        "  image = preprocess_input(image)\n",
        "\n",
        "# Применение сети VGG16 для классификации изображения\n",
        "  predictions = model.predict(image)\n",
        "\n",
        "# Декодирование предсказаний\n",
        "  label = decode_predictions(predictions)\n",
        "\n",
        "# Вывод результатов\n",
        "  print(file+': '+label[0][0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNPh3mQptg4g",
        "outputId": "026215c6-2909-4408-bd2a-33c9085e837d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 746ms/step\n",
            "dog.jpg: collie\n",
            "1/1 [==============================] - 1s 585ms/step\n",
            "cat.jpg: tabby\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "JG2u8a-OmBRf"
      }
    }
  ]
}